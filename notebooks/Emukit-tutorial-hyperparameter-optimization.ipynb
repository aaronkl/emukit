{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Optimization with Emukit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports and parameters of figures should be loaded at the beginning of the overview\n",
    "\n",
    "### General imports\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors as mcolors\n",
    "\n",
    "### --- Figure config\n",
    "colors = dict(mcolors.BASE_COLORS, **mcolors.CSS4_COLORS)\n",
    "LEGEND_SIZE = 15\n",
    "TITLE_SIZE = 25\n",
    "AXIS_SIZE = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we will learn how we can use Bayesian optimization to tune the hyperparameters of a small feed forward neural network on MNIST. We will also show how we can speed up the optimization process by reasoning across training dataset subsets.\n",
    "\n",
    "Before you start, make sure that you installed the following dependencies:\n",
    "- numpy (pip install numpy)\n",
    "- matplotlib (pip install matplotlib)\n",
    "- keras (pip install keras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Navigation\n",
    "\n",
    "1. [Section 1](#1.-Section-1)\n",
    "\n",
    "2. [Section 2](#2.-Section-2)\n",
    "\n",
    "3. [Conclusions](#3.-Conclusions)\n",
    "\n",
    "4. [References](#4.-References)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Continuous Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we learn how we can use Emukit to optimize the learning rate, batch size and the dropout rates of a 2-layer feed-forward neural network on MNIST. For simplicity we bootstrap from the MNIST [example](https://github.com/keras-team/keras/blob/master/examples/mnist_mlp.py) of Keras.  \n",
    "The code implements the objective function, which get as input a hyperparameter configuration, trains the network with these hyperparameters and finally returns the validation error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "\n",
    "def train_and_validate(hyperparameters):\n",
    "    print(hyperparameters)\n",
    "    return np.random.randn(1,1)\n",
    "    batch_size = 128\n",
    "    num_classes = 10\n",
    "    epochs = 20\n",
    "\n",
    "    # the data, split between train and test sets\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "    x_train = x_train.reshape(60000, 784)\n",
    "    x_test = x_test.reshape(10000, 784)\n",
    "    x_train = x_train.astype('float32')\n",
    "    x_test = x_test.astype('float32')\n",
    "    x_train /= 255\n",
    "    x_test /= 255\n",
    "    print(x_train.shape[0], 'train samples')\n",
    "    print(x_test.shape[0], 'test samples')\n",
    "\n",
    "    # convert class vectors to binary class matrices\n",
    "    y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "    y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, activation='relu', input_shape=(784,)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=RMSprop(),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    history = model.fit(x_train, y_train,\n",
    "                        batch_size=batch_size,\n",
    "                        epochs=epochs,\n",
    "                        verbose=1,\n",
    "                        validation_data=(x_test, y_test))\n",
    "    score = model.evaluate(x_test, y_test, verbose=0)\n",
    "    \n",
    "    return 1 - score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To allow for an easy integration into the emukit, we wrap the objective function with the UserFunction interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from emukit.core.loop.user_function import UserFunctionWrapper\n",
    "\n",
    "objective_function = UserFunctionWrapper(train_and_validate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides the objective funtion, we need to define the configuration space, or input space which defines the ranges for all hyperparameters. \n",
    "Note that, it is usually good practice to optimize the learning rate on a logarithmic scale rather than a linear scale, thus, we optimize the exponent $\\gamma \\in [-6, -1]$ and set the learning rate in the objective function $10^{\\gamma}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from emukit.core import ContinuousParameter, ParameterSpace\n",
    "\n",
    "list_params = []\n",
    "list_params.append(ContinuousParameter(\"learning_rate\",-6, -1))\n",
    "list_params.append(ContinuousParameter(\"batch_size\",32, 512))\n",
    "list_params.append(ContinuousParameter(\"dropout_1\", 0, .99))\n",
    "list_params.append(ContinuousParameter(\"dropout_2\", 0, .99))\n",
    "\n",
    "configuration_space = ParameterSpace(list_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Random Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get started, we will first look at random search [[Bergstra et al, 2012]](#4.-References), which is, besides its simplicity, usually a quite tough baseline to beat. In general, if you develop a new hyperparameter optimization method, you should always first compare against random search to see whether your method actually works or if there is still a bug somewhere."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function draws `n_iters` hyperparameter configurations uniformly at random from the configuration space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-5.57514487e+00  4.57510122e+02  8.57193240e-01  3.65013894e-01]]\n",
      "[[ -1.93383353 261.44480403   0.52089088   0.68185324]]\n",
      "[[ -1.65343417 279.5912788    0.63661122   0.71218883]]\n",
      "[[-2.83757350e+00  4.40945941e+02  1.10851391e-01  9.71881406e-01]]\n",
      "[[-4.22356587e+00  4.65442396e+02  3.86356272e-01  1.04874499e-01]]\n"
     ]
    }
   ],
   "source": [
    "from emukit.benchmarking.loop_benchmarking.random_search import RandomSearch\n",
    "rs = RandomSearch(configuration_space)\n",
    "rs.run_loop(objective_function, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Bayesian optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we familiarized ourself with the problem, we will now look at how we can use Emukit's Bayesian optimization module to optimize the hyperparameters.\n",
    "For more details about Bayesian optimization and how it is implemented in Emukit have a look at this [tutorial](https://github.com/amzn/emukit/blob/master/notebooks/Emukit-tutorial-Bayesian-optimization-introduction.ipynb) here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayesian optimization uses a model to guide the search. However, to train a model we first need data, which we do not have in the beginning. Thus, we will first use a so called initial design to collect some data point before we start the Bayesian optimization loop. Here we will simply draw and evaluate N random configuration. However, one could do more sophisticated things such as es experimental design or multi-task learning to warmstart the optimization procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'configuration_space' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-9d5d73b75c72>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mn_init\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0minitial_design\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomDesign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfiguration_space\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mX_init\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitial_design\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_init\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'configuration_space' is not defined"
     ]
    }
   ],
   "source": [
    "from emukit.experimental_design.model_free.random_design import RandomDesign\n",
    "n_init = 2\n",
    "\n",
    "initial_design = RandomDesign(configuration_space)\n",
    "\n",
    "X_init = initial_design.get_samples(n_init)\n",
    "Y_init = np.zeros([n_init, 1])\n",
    "for i in range(n_init):\n",
    "    Y_init[i] = objective_function.evaluate(X_init[i, None])[0].Y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define all the ingredients to run the main loop. Since all hyperparameters are continuous we use a Gaussian process (GP) as probabilistic model of the objective function and expected improvement as acquisition function. Following [[Snoek et al, 2012]](#4.-References) we integrate the acquisition function over the GP hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import GPy\n",
    "\n",
    "from emukit.bayesian_optimization.acquisitions import ExpectedImprovement\n",
    "from emukit.bayesian_optimization.loops import BayesianOptimizationLoop\n",
    "from emukit.core.acquisition import IntegratedHyperParameterAcquisition\n",
    "from emukit.model_wrappers import GPyModelWrapper\n",
    "\n",
    "kernel = GPy.kern.Matern52(X_init.shape[1], variance=1., ARD=False)\n",
    "gpmodel = GPy.models.GPRegression(X_init, Y_init, kernel)\n",
    "\n",
    "model = GPyModelWrapper(gpmodel)\n",
    "\n",
    "acquisition_generator = lambda model: ExpectedImprovement(model)\n",
    "acquisition = IntegratedHyperParameterAcquisition(model, acquisition_generator)\n",
    "\n",
    "bo =  BayesianOptimizationLoop(space=configuration_space, model=model, acquisition=acquisition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Now we are all set and can "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization restart 1/1, f = 4.57633697608823\n",
      "[[-4.47031553e+00  5.11556372e+02  4.34035070e-01  2.76702315e-01]]\n",
      "Optimization restart 1/1, f = 6.257839491834606\n",
      "[[-4.29247068 32.42386657  0.2527486   0.5821326 ]]\n",
      "Optimization restart 1/1, f = 7.9521390370070915\n",
      "[[ -5.34278582 233.57920905   0.29280434   0.97328834]]\n",
      "Optimization restart 1/1, f = 9.520802955249184\n",
      "[[-1.35853062e+00  4.52192784e+02  9.07874679e-02  6.57912882e-01]]\n",
      "Optimization restart 1/1, f = 11.22066770216205\n",
      "[[-1.35288357e+00  4.52197706e+02  8.95969815e-02  6.57448270e-01]]\n",
      "Optimization restart 1/1, f = 12.619679708480689\n"
     ]
    }
   ],
   "source": [
    "bo.run_loop(objective_function, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Fabolas: Fast Bayesian Optimization on Large Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def train_and_validate_dataset_size(hyperparameters, dataset_size):\n",
    "    print(hyperparameters, dataset_size)\n",
    "    return np.random.randn(1,1)\n",
    "\n",
    "    start_time = time.time()\n",
    "    batch_size = 128\n",
    "    num_classes = 10\n",
    "    epochs = 20\n",
    "\n",
    "    # the data, split between train and test sets\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "    x_train = x_train.reshape(60000, 784)\n",
    "    x_test = x_test.reshape(10000, 784)\n",
    "    x_train = x_train.astype('float32')\n",
    "    x_test = x_test.astype('float32')\n",
    "    x_train /= 255\n",
    "    x_test /= 255\n",
    "    print(x_train.shape[0], 'train samples')\n",
    "    print(x_test.shape[0], 'test samples')\n",
    "\n",
    "    # convert class vectors to binary class matrices\n",
    "    y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "    y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, activation='relu', input_shape=(784,)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=RMSprop(),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    history = model.fit(x_train, y_train,\n",
    "                        batch_size=batch_size,\n",
    "                        epochs=epochs,\n",
    "                        verbose=1,\n",
    "                        validation_data=(x_test, y_test))\n",
    "    score = model.evaluate(x_test, y_test, verbose=0)\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    return 1 - score, training_time\n",
    "    \n",
    "    \n",
    "    return np.array([[y]]), np.array([[c]])\n",
    "\n",
    "s_min = 100\n",
    "s_max = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LatinDesign' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-91203976bc18>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0minitial_design\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLatinDesign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mgrid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitial_design\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_init\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'LatinDesign' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# initial_design = LatinDesign(space)\n",
    "\n",
    "# grid = initial_design.get_samples(n_init)\n",
    "# X_init = np.zeros([n_init, grid.shape[1] + 1])\n",
    "# Y_init = np.zeros([n_init, 1])\n",
    "# cost_init = np.zeros([n_init])\n",
    "\n",
    "# subsets = np.array([s_max // 2 ** i for i in range(2, 10)])[::-1]\n",
    "# idx = np.where(subsets < s_min)[0]\n",
    "\n",
    "# subsets[idx] = s_min\n",
    "\n",
    "# for it in range(n_init):\n",
    "#     func_val, cost = func(x=grid[it], s=subsets[it % len(subsets)])\n",
    "\n",
    "#     X_init[it] = np.concatenate((grid[it], np.array([subsets[it % len(subsets)]])))\n",
    "#     Y_init[it] = func_val\n",
    "#     cost_init[it] = cost\n",
    "\n",
    "\n",
    "# loop = FabolasLoop(X_init=X_init, Y_init=Y_init, cost_init=cost_init, space=space, s_min=s_min,\n",
    "#                    s_max=s_max, marginalize_hypers=marginalize_hypers)\n",
    "loop = FabolasLoop(X_init=None, Y_init=None, cost_init=None, space=ParameterSpacespace, s_min=s_min,\n",
    "                   s_max=s_max, marginalize_hypers=marginalize_hypers)\n",
    "\n",
    "loop.run_loop(user_function=UserFunctionWrapper(train_and_validate_dataset_size),\n",
    "              stopping_condition=FixedIterationsStoppingCondition(n_iters - n_init))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Discrete Hyperparameter Optimization and Architecture Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Kennedy, M.C. and O'Hagan, A., 2000. *Predicting the output from a complex computer code when fast approximations are available.* Biometrika, 87(1), pp.1-13."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
